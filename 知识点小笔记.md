# 知识点小笔记

- python中‘...’是什么？

Ellipsis expand to the number of : objects needed to make a selection tuple of the same length as x.ndim. 
Only the first ellipsis is expanded, any others are interpreted as :. 
Example

```python
>>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
>>> x[...,0]
array([[1, 2, 3],
[4, 5, 6]])
```

- 图形语义分割中的上采样

  最简单的Decoder只使用Upsampling ,可能最终生成一个{0,1,2,3...,C} <sup>HxW</sup>形的特征图，C和C～为分割类别数，以及最终输出的通道数。Y通常会使用one-hot编码，{0,1,2,3...,C}<sup>H*W</sup>,

  ![1554363078767](/home/leixiangyu/.config/Typora/typora-user-images/1554363078767.png)

  - 深度可分卷积

    the depthwise convolution performs a spatial convolution independently for each input channel, while the pointwise convolution is employed to combine the output from the depthwise convolution.

- Accurate semantic image segmentation requires the joint consideration of local appearance, semantic information, and global scene context.

- Feature Connection : Connections between feature maps enable the communication between neurons with different receptive field sizes, yielding new feature  maps that encode multi-scale context information. Basically, FCN-based models use separate neurons to represent the regular regions in an image. Normally, they use convolutional/ pooling kernels with predefined shapes to aggregate the information of adjacent neurons, and propagate this information to   the neurons of other feature maps. But traditional convolutional/pooling kernels   only capture the context information in a local scale. To leverage richer context   information, graphical models are integrated with FCNs. Graphical   models build dense connections between feature maps, allowing neurons to be   more sensitive to the global image content that is critical for learning good   segmentation features. Note that previous works use one-way connections that   extract context information from the feature maps separately, which is eventually combined. Thus, the learned features at a given scale are not given the   opportunity to optimally account for the multi-scale context information from   all of the other scales.   In contrast to previous methods, our bidirectional connections exchange   multi-scale context information to improve the learning of all features. We employ super-pixels computed based on the image structure, and use the relationship between them to define the exchange routes between neurons in different feature maps. This enables more adaptive context information propagation. Several  previous works [31–33, 36] also use super-pixels to define the feature connections.   And information exchange has been studied in [37, 38] for object detection. But these works do not exchange the information between feature maps of different resolutions, which is critical for semantic segmentation.

- pytorch

  1. Q:ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).python: symbol lookup error: Rerunning with num_workers=0 may give better error trace.

     A:JUST DO AS THE LOG SAID；try to start your docker container with `--ipc=host`

  2. Q:THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument

     A:2080ti需要用cuda10

     B：pip3 install -U https://download.pytorch.org/whl/cu100/torch-1.0.0-cp36-cp36m-linux_x86_64.whl TO 

- ImportError: libx264.so.138: cannot open shared object file: No such file or directory

- TypeError: **new**() got an unexpected keyword argument 'serialized_options'

  conda 环境protobuf版本与系统的不相同。bash下通过protoc --version查看，conda list | grep protobu

- opencv libstdc++.so.6: version `GLIBCXX_3.4.22' not found

  https://blog.csdn.net/pursuit_zhangyu/article/details/79450027

- 常用衡量指标

  机器学习评价指标合辑（F1score/P-R曲线/ROC曲线/AUC) - 橙煦猿的文章 - 知乎 https://zhuanlan.zhihu.com/p/60218684 

- 神经网络设计法则

  1. 避免表达瓶颈，尤其是网络早期。前馈网络在向后传递信息时，通过瓶颈层（高度压缩的层）时，会损失大量的有用信息，也就是遇到了表达瓶颈。当信息向后传播时，特征尺寸一般是要逐渐减小的，但是不要急剧减少，比如feature map直接从128x128减小到3x3，这样的网络是很难训练的。另外，特征维度（通道数）一般会逐渐增加，但是维度只是作为一种估计的手段，并不代表信息的多少，因为会丢弃相关结构等重要信息。

  2. 高维特征更易处理。高维特征（多次非线性映射）带有更多的判别信息，网络也会更容易训练。

  3. 可以在低维特征时进行空间聚合，而不必担心会损失太多信息。这句话的意思是在网络前几层的时候可以考虑降低特征维度，不会损失太多信息，反而还能加速训练。例如，在低维特征时，我们在进行3x3卷积前，可以先使用pooling操作，加速训练。

  4. 平衡网络的宽度与深度。过深或过宽的网络都会使网络性能达不到最优。
